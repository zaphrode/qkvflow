# Time-Indexed Parameter Sharing for Neural ODE Transformers

**A Novel Approach for Efficient Transformers with 9.1% Better Performance and 62-431Ã— Parameter Compression**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![JAX](https://img.shields.io/badge/JAX-0.4.28-orange.svg)](https://github.com/google/jax)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ðŸŽ¯ Key Results

| Model | Valid Loss | Parameters | Speed | vs ICLR 2025 |
|-------|------------|------------|-------|--------------|
| **Time-Indexed SSM** | **2.147 Â± 0.124** | 4.9M | 64.3 ms/step | **9.1% better** âœ… |
| **Time-Indexed MLP** | **2.231 Â± 0.025** | 0.7M | 7.7 ms/step | **4.5% better** âœ… |
| Tong's Neural ODE (baseline) | 2.336 Â± 0.018 | 51.5M | 15.3 ms/step | â€” |
| Standard Transformer | 2.367 Â± 0.022 | 308.5M | 55.3 ms/step | â€” |

*Results on WikiText-2, averaged over 5 random seeds with 95% confidence intervals.*

**Statistical Significance:** All improvements are statistically significant (p < 0.05)

---

## ðŸš€ What's New?

**Core Insight:** Constrained parameter sharing with time-indexed modulation outperforms unrestricted time-dependent weight generation.

Instead of generating entire weight matrices at each layer:
```python
W(t) = HyperNetwork(time_embed(t))  # Tong's approach (51M params)
```

We use **shared base weights** with **lightweight time modulation**:
```python
W(t) = W_base âŠ™ Ïƒ(MLP_small(time_embed(t)))  # Ours (0.7-4.9M params)
```

**Benefits:**
- âœ… **Implicit regularization** through shared structure
- âœ… **Easier optimization** (smaller search space)
- âœ… **Better generalization** (validated with error bars)
- âœ… **Practical efficiency** (430Ã— compression, 7.2Ã— speedup)

---

## ðŸ“Š Results & Figures

### Performance Comparison
![Statistical Performance](publication_figures/statistical_performance.png)

### Parameter Efficiency
![Efficiency](publication_figures/efficiency_with_error.png)

### Speed Comparison
![Speed](publication_figures/speed_comparison.png)

### Statistical Significance
![Significance Tests](publication_figures/significance_tests.png)

*All figures include error bars from 5 random seeds.*

---

## ðŸ—ï¸ Architecture

### Standard Transformer (Baseline)
- Separate weight matrices for each layer
- 308M parameters for 6 layers

### Tong's Neural ODE (ICLR 2025)
- Hypernetwork generates all weights from time embedding
- 51M parameters (independent of depth)

### Ours: Time-Indexed Parameter Sharing
- **Shared base weights** across all layers
- **Lightweight modulation** network (64 hidden units)
- **Two variants:**
  - **MLP**: 0.7M parameters (430Ã— compression)
  - **SSM**: 4.9M parameters (62Ã— compression)

---

## ðŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/zaphrode/qkvflow.git
cd qkvflow

# Create virtual environment
python3.11 -m venv venv311
source venv311/bin/activate

# Install dependencies
pip install -r requirements.txt

# For GPU support (CUDA 12.x)
pip install --upgrade "jax[cuda12]"
```

---

## ðŸš€ Quick Start

### Run Statistical Validation (5 Seeds)
```bash
python scripts/run_5_seed_validation.py
```

### Compare All Models
```bash
python scripts/compare_vs_tong_neuralode.py
```

### Generate Publication Figures
```bash
python scripts/plot_statistical_results.py
```

---

## ðŸ“ˆ Reproduce Our Results

### 1. Statistical Validation
```bash
# Run all models with 5 different seeds
python scripts/run_5_seed_validation.py

# Output: statistical_validation_results/
# - statistics_summary.json
# - significance_tests.json
# - seed_*.pkl (individual results)
```

### 2. Generate Figures
```bash
# Create publication-quality plots with error bars
python scripts/plot_statistical_results.py

# Output: publication_figures/
# - statistical_performance.png/pdf
# - efficiency_with_error.png/pdf
# - speed_comparison.png/pdf
# - significance_tests.png/pdf
# - results_table.tex (LaTeX table)
```

---

## ðŸ”¬ Key Contributions

1. **Novel Architecture**: Time-indexed parameter sharing with constrained modulation
2. **Strong Empirical Results**: 9.1% better than ICLR 2025 with 62Ã— fewer parameters
3. **Theoretical Insight**: Why constrained sharing beats unrestricted generation
4. **Statistical Validation**: All results with error bars (5 seeds)
5. **Two Variants**: MLP (extreme compression) and SSM (best performance)

---

## ðŸ“Š Detailed Results

### Performance (Validation Loss)

| Model | Mean | Std | 95% CI | p-value vs Tong |
|-------|------|-----|--------|-----------------|
| Time-Indexed SSM | 2.147 | 0.124 | [1.974, 2.319] | **0.017** |
| Time-Indexed MLP | 2.231 | 0.025 | [2.196, 2.265] | **0.0001** |
| Tong's Neural ODE | 2.336 | 0.018 | [2.311, 2.361] | â€” |
| Standard | 2.367 | 0.022 | [2.337, 2.398] | 0.062 |

### Parameter Efficiency

| Model | Parameters | Compression | Params/Loss |
|-------|------------|-------------|-------------|
| Time-Indexed MLP | **0.7M** | **430.9Ã—** | **0.32M** |
| Time-Indexed SSM | **4.9M** | **62.9Ã—** | **2.38M** |
| Tong's Neural ODE | 51.5M | 6.0Ã— | 22.3M |
| Standard | 308.5M | 1.0Ã— | 130.4M |

### Speed (Training Time per Step)

| Model | Mean (ms) | Std | Speedup |
|-------|-----------|-----|---------|
| **Time-Indexed MLP** | **7.7** | 0.3 | **7.2Ã—** |
| Tong's Neural ODE | 15.3 | 0.1 | 3.6Ã— |
| Standard | 55.3 | 1.2 | 1.0Ã— |
| Time-Indexed SSM | 64.3 | 0.5 | 0.9Ã— |

---

## ðŸ§® Mathematical Formulation

### Standard Transformer
```
x_{i+1} = x_i + Attention(x_i; W^Q_i, W^K_i, W^V_i) + FFN(x_i; W^{up}_i, W^{down}_i)
```
**Parameters:** O(L Â· dÂ²) where L is depth

### Tong's Neural ODE
```
W(t) = g_Î¸(SinusoidalEmbed(t))
x_{i+1} = x_i + f(x_i, t_i; W(t_i)) Â· Î”t
```
**Parameters:** O(dÂ² Â· h) (independent of L)

### Ours: Time-Indexed Sharing
```
W_eff(t) = W_base âŠ™ Ïƒ(MLP_Ï†(SinusoidalEmbed(t)))
x_{i+1} = x_i + f(x_i, t_i; W_eff(t_i))
```
**Parameters:** O(dÂ² + d Â· h_mod) where h_mod â‰ª d

**Key Difference:** We constrain to a low-dimensional manifold through shared base weights.

---

## ðŸ“š Citation

```bibtex
@article{qkvflow2025,
  title={Time-Indexed Parameter Sharing for Neural ODE Transformers},
  author={[Your Name]},
  year={2025},
  note={In preparation}
}
```

---

## ðŸ—‚ï¸ Project Structure

```
qkvflow/
â”œâ”€â”€ config/                      # Model configurations
â”‚   â”œâ”€â”€ neuralode_config.py
â”‚   â””â”€â”€ neuralode_ssm_config.py
â”œâ”€â”€ qkvflow/                     # Core implementation
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ neuralode_lm.py     # MLP variant
â”‚   â”‚   â””â”€â”€ neuralode_ssm_lm.py # SSM variant
â”‚   â””â”€â”€ nn/                      # Neural network modules
â”œâ”€â”€ scripts/                     # Experiment scripts
â”‚   â”œâ”€â”€ run_5_seed_validation.py
â”‚   â”œâ”€â”€ compare_vs_tong_neuralode.py
â”‚   â””â”€â”€ plot_statistical_results.py
â”œâ”€â”€ publication_figures/         # Publication-ready plots
â”œâ”€â”€ statistical_validation_results/ # Statistical data
â”œâ”€â”€ RESEARCH_SUMMARY.md         # Full paper draft
â”œâ”€â”€ PUBLICATION_ROADMAP.md      # Publication guide
â””â”€â”€ requirements.txt
```

---

## ðŸŽ“ For Researchers

### Paper Draft
See `RESEARCH_SUMMARY.md` for the full paper with:
- Abstract and introduction
- Mathematical formulation
- Detailed experimental setup
- Results and analysis
- Discussion and related work

### LaTeX Table
Ready-to-use LaTeX table in `publication_figures/results_table.tex`

### Statistical Data
All raw data in `statistical_validation_results/`

---

## ðŸ”§ Requirements

- Python 3.11+
- JAX 0.4.28+ (with CUDA support for GPU)
- Equinox 0.11.4+
- Haliax 1.3+
- NumPy, SciPy, Matplotlib, Seaborn

See `requirements.txt` for complete list.

---

## ðŸ™ Acknowledgments

This work builds upon:
- **Tong et al. (ICLR 2025)**: Neural ODE Transformers
- **Gu & Dao (2023)**: Mamba (Selective State Space Models)
- **Chen et al. (2018)**: Neural Ordinary Differential Equations

---

## ðŸ“„ License

MIT License - See LICENSE file for details

---

## ðŸ› Issues & Contributions

Found a bug or want to contribute? Please open an issue or pull request!

---

## ðŸ“§ Contact

For questions about the research, please open an issue or contact: [your-email]

---

## â­ Star this repository if you find it useful!

**Publication Status:** Manuscript in preparation for NeurIPS/ICML 2026

---

*Last updated: December 2025*
