# Neural ODE + SSM Integration Guide

This guide explains how to use the SSM (State Space Model) variant of the Neural ODE Transformer.

## Overview

We've replaced the Feed-Forward Network (FFN) in the Neural ODE Transformer with a time-varying Structured State Space Model (SSM). This should provide:

- **Faster inference**: Linear complexity vs quadratic for long sequences
- **Fewer parameters**: ~60% of MLP parameters for same hidden dimension
- **Better long-context**: SSMs handle long sequences more efficiently

## Architecture

```
Standard Neural ODE: áº‹(t) = f_attn(x,t) + g_ffn(x,t)
SSM Variant:         áº‹(t) = f_attn(x,t) + g_ssm(x,t)
```

Where `g_ssm(x,t)` is a time-varying SSM with parameters A(t), B(t), C(t), D(t), Î”(t) generated by a hypernetwork.

## Quick Start

### 1. Verify Installation

First, test that the SSM model works correctly:

```bash
python scripts/test_ssm_model.py
```

Expected output:
```
ğŸ‰ ALL TESTS PASSED! Model is ready for training.
```

### 2. Train SSM Model

Train the SSM variant:

```bash
python scripts/train_neuralode_ssm.py \
    --model ssm \
    --config small \
    --ssm_state_size 64 \
    --max_tokens 10_000_000 \
    --batch_size 8 \
    --learning_rate 3e-4
```

### 3. Train Baseline (MLP) for Comparison

Train the standard MLP version:

```bash
python scripts/train_neuralode_ssm.py \
    --model mlp \
    --config small \
    --max_tokens 10_000_000 \
    --batch_size 8 \
    --learning_rate 3e-4
```

## Configuration Options

### Model Sizes

Three preset configurations:

```python
# Small (~19M params)
python scripts/train_neuralode_ssm.py --config small

# Medium (~100M params)  
python scripts/train_neuralode_ssm.py --config medium

# Large (~350M params)
python scripts/train_neuralode_ssm.py --config large
```

### SSM-Specific Parameters

```bash
--ssm_state_size 64        # SSM hidden state dimension (32/64/128/256)
--time_embedding_dim 64    # Time embedding dimension
--sinusoidal_dim 32        # Sinusoidal encoding dimension
```

### Key Training Parameters

```bash
--batch_size 8             # Batch size per device
--learning_rate 3e-4       # Peak learning rate
--warmup_steps 2000        # Warmup steps
--weight_decay 0.01        # Weight decay
--grad_clip 1.0            # Gradient clipping

--eval_every 1000          # Evaluate every N steps
--save_every 5000          # Save checkpoint every N steps
--log_every 10             # Log metrics every N steps
```

## Parameter Comparison

Small config (hidden_dim=256, 6 layers):

| Component | SSM Params | MLP Params | Reduction |
|-----------|-----------|-----------|-----------|
| Embeddings | ~13M | ~13M | 0% |
| Attention | ~6M | ~6M | 0% |
| FFN/SSM | ~1M | ~32M | **-97%** |
| **Total** | **~19M** | **~51M** | **-63%** |

## Ablation Studies

### SSM State Size

Test different SSM hidden dimensions:

```bash
# Smaller state (faster, fewer params)
python scripts/train_neuralode_ssm.py --ssm_state_size 32

# Medium state (balanced)
python scripts/train_neuralode_ssm.py --ssm_state_size 64

# Larger state (more capacity)
python scripts/train_neuralode_ssm.py --ssm_state_size 128

# Very large state
python scripts/train_neuralode_ssm.py --ssm_state_size 256
```

### Fair Comparison (Matched Parameters)

To fairly compare SSM vs MLP, match parameter counts by adjusting hidden dimensions:

```bash
# SSM: 19M params (hidden_dim=256)
python scripts/train_neuralode_ssm.py \
    --model ssm \
    --config small

# MLP: 19M params (reduce mlp_ratio from 4.0 to ~2.0)
# TODO: Implement mlp_ratio parameter
```

## Expected Performance

Based on SSM properties:

### SSM Should Win:
- âœ… **Long sequences** (>2K tokens): Linear complexity
- âœ… **Memory efficiency**: Smaller model size
- âœ… **Inference speed**: Fewer parameters

### SSM Might Struggle:
- âŒ **Short sequences** (<512 tokens): Overhead not worth it
- âŒ **Token-level tasks**: Less expressive than attention
- âŒ **Initial training**: May need more warmup

## Evaluation Metrics

Track these metrics during training:

```python
# Primary metric
perplexity = jnp.exp(loss)

# Efficiency metrics
tokens_per_second = batch_size * seq_len / step_time
memory_usage = jax.device_get(jax.live_arrays())

# Long-context evaluation
eval_perplexity_2k = evaluate(model, dataset, max_length=2048)
eval_perplexity_4k = evaluate(model, dataset, max_length=4096)
```

## Debugging Tips

### NaN Losses

If you see NaN losses:
1. Decrease learning rate: `--learning_rate 1e-4`
2. Increase warmup: `--warmup_steps 5000`
3. Reduce gradient clipping: `--grad_clip 0.5`
4. Check SSM initialization (A should be stable)

### Poor Performance

If SSM significantly underperforms MLP:
1. Increase SSM state size: `--ssm_state_size 128`
2. Check time embedding dimensionality
3. Verify SSM parameters vary with time (run test script)
4. Try different initialization for A matrix

### Memory Issues

If you hit OOM:
1. Reduce batch size: `--batch_size 4`
2. Enable gradient checkpointing
3. Use mixed precision training
4. Reduce sequence length

## Next Steps

### Integration Checklist

- [x] Implement `TemporalSSM` module
- [x] Create `SSMBlock` (attention + SSM)
- [x] Build `NeuralOdeSSMLMHeadModel`
- [x] Write training script
- [x] Create test suite
- [ ] **Connect data pipeline** (TODO: Critical!)
- [ ] Add evaluation harness
- [ ] Implement checkpointing/resuming
- [ ] Add Weights & Biases logging
- [ ] Long-context evaluation (2K/4K/8K tokens)

### Immediate TODOs

**Priority 1**: Connect data pipeline
```python
# In train_neuralode_ssm.py, replace dummy batch generation with:
from levanter.data.text import LMDatasetConfig, TokenSeqDataset

dataset_config = LMDatasetConfig(
    id=args.dataset,
    cache_dir=args.cache_dir,
)
train_dataset = TokenSeqDataset(dataset_config, split="train")
```

**Priority 2**: Add evaluation loop
```python
def evaluate(model, eval_dataset, num_batches):
    total_loss = 0.0
    for batch in eval_dataset.take(num_batches):
        loss = eval_step(model, batch, key)
        total_loss += loss
    return total_loss / num_batches
```

**Priority 3**: Add checkpointing
```python
# Save with metadata
checkpoint = {
    'model': model,
    'opt_state': opt_state,
    'step': step,
    'config': config,
}
eqx.tree_serialise_leaves(checkpoint_path, checkpoint)
```

## File Structure

```
qkvflow/
â”œâ”€â”€ qkvflow/models/
â”‚   â””â”€â”€ neuralode_ssm_lm.py          # SSM model implementation
â”œâ”€â”€ config/
â”‚   â””â”€â”€ neuralode_ssm_config.py      # Configuration classes
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_neuralode_ssm.py       # Training script
â”‚   â””â”€â”€ test_ssm_model.py            # Validation tests
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ SSM_INTEGRATION.md           # This file
â””â”€â”€ notebooks/
    â””â”€â”€ diffeqformer_ssm.ipynb       # Initial prototype
```

## References

- Neural ODE Transformers (ICLR 2025)
- Mamba: Linear-Time Sequence Modeling with Selective State Spaces
- AI21 Jamba: Hybrid Attention-SSM Architecture
- S4: Structured State Space Sequence Models

## Support

If you encounter issues:
1. Run `python scripts/test_ssm_model.py` to verify installation
2. Check gradient norms for NaN/explosion
3. Review SSM parameter evolution during training
4. Compare against baseline MLP model

For questions, create an issue with:
- Full error message/traceback
- Training configuration used
- Output from test script
- System info (JAX version, GPU type)

